{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: first do face detection and face recognition then use CNN's models(VGGFace, FaceNet, ResNet). which have more accuracy use that. training set or directory is Sree, test test is Tester.\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Function for face detection\n",
        "def detect_faces(image_path):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    detected_faces = []\n",
        "    for (x, y, w, h) in faces:\n",
        "        detected_faces.append(img[y:y + h, x:x + w])  # Extract face region\n",
        "    return detected_faces\n",
        "\n",
        "\n",
        "# Function to preprocess image data for CNN models\n",
        "def preprocess_image(image):\n",
        "  image = cv2.resize(image, (224, 224))  # Adjust size as per model input\n",
        "  image = image / 255.0  # Normalize pixel values\n",
        "  return image\n",
        "\n",
        "\n",
        "# Load and preprocess data\n",
        "train_dir = \"/content/sample_data/Sree\"  # Replace with your training directory\n",
        "test_dir = \"/content/sample_data/Tester\" # Replace with your test directory\n",
        "train_data = []\n",
        "train_labels = []\n",
        "test_data = []\n",
        "test_labels = []"
      ],
      "metadata": {
        "id": "6CIlcPxGNLlA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data preparation\n",
        "for item in os.listdir(train_dir):\n",
        "    item_path = os.path.join(train_dir, item)\n",
        "    # Check if the item is a directory\n",
        "    if os.path.isdir(item_path):\n",
        "        # If it's a directory, treat it as a label and process images inside it\n",
        "        label = item\n",
        "        for filename in os.listdir(item_path):\n",
        "            image_path = os.path.join(item_path, filename)\n",
        "            faces = detect_faces(image_path)\n",
        "            for face in faces:\n",
        "                preprocessed_face = preprocess_image(face)\n",
        "                train_data.append(preprocessed_face)\n",
        "                train_labels.append(label)\n",
        "    else:\n",
        "        # If it's a file (likely an image), process it directly\n",
        "        image_path = item_path\n",
        "        faces = detect_faces(image_path)\n",
        "        for face in faces:\n",
        "            preprocessed_face = preprocess_image(face)\n",
        "            train_data.append(preprocessed_face)\n",
        "            train_labels.append(item)"
      ],
      "metadata": {
        "id": "k4nNyc0XR29C"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data preparation\n",
        "test_data = []  # Initialize test_data as a list\n",
        "test_labels = []\n",
        "for item in os.listdir(test_dir):\n",
        "    item_path = os.path.join(test_dir, item)\n",
        "    # Check if the item is a directory\n",
        "    if os.path.isdir(item_path):\n",
        "        # If it's a directory, treat it as a label and process images inside it\n",
        "        label = item\n",
        "        for filename in os.listdir(item_path):\n",
        "            image_path = os.path.join(item_path, filename)\n",
        "            faces = detect_faces(image_path)\n",
        "            for face in faces:\n",
        "                preprocessed_face = preprocess_image(face)\n",
        "                test_data.append(preprocessed_face) # Append to the list\n",
        "                test_labels.append(label)\n",
        "    else:\n",
        "        # If it's a file (likely an image), process it directly\n",
        "        image_path = item_path\n",
        "        faces = detect_faces(image_path)\n",
        "        for face in faces:\n",
        "            preprocessed_face = preprocess_image(face)\n",
        "            test_data.append(preprocessed_face) # Append to the list\n",
        "            test_labels.append(item)"
      ],
      "metadata": {
        "id": "21xC8iNQTaN7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to NumPy arrays\n",
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)\n",
        "test_data = np.array(test_data)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "-KTGXDInTO68"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets (optional but recommended)\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "eHw4X9F-TkTy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model (VGG16 or ResNet50)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom classification layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)  # Adjust units as needed\n",
        "predictions = Dense(len(np.unique(train_labels)), activation='softmax')(x)  # Output layer\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the pre-trained layers (optional, but often helpful for fine-tuning)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FALqga4TTwK3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ... (your existing code for data loading and preprocessing) ...\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on all unique labels (from both train and validation sets)\n",
        "all_labels = np.concatenate((y_train, y_val), axis=0)\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# Transform your labels to numerical values\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "\n",
        "# Handle unseen labels in test data\n",
        "# Instead of directly transforming test_labels, iterate through them and handle unknown labels\n",
        "test_labels_encoded = []\n",
        "for label in test_labels:\n",
        "    try:\n",
        "        encoded_label = label_encoder.transform([label])[0]  # Encode if known\n",
        "    except ValueError:\n",
        "        encoded_label = -1  # Assign a special value for unknown labels (e.g., -1)\n",
        "        print(f\"Warning: Unseen label '{label}' in test data. Assigned -1.\")\n",
        "    test_labels_encoded.append(encoded_label)\n",
        "\n",
        "test_labels = np.array(test_labels_encoded)\n",
        "\n",
        "# ... (rest of your model training and evaluation code) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF6uHHviVUAy",
        "outputId": "d3fc77a9-55fb-4e32-8bba-3057352a0860"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Unseen label 'prudvi photo.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label '1729165320963.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label '1729165320915.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label '1729165320915.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label 'passport Size photo.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label 'passport Size photo.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label 'passport Size photo.jpg' in test data. Assigned -1.\n",
            "Warning: Unseen label 'photomode_17092024_155342.png' in test data. Assigned -1.\n",
            "Warning: Unseen label 'photomode_17092024_155342.png' in test data. Assigned -1.\n",
            "Warning: Unseen label 'photomode_17092024_155342.png' in test data. Assigned -1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val)) # Adjust epochs and batch size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEjgUJXrU53W",
        "outputId": "4319214f-02b2-4e2e-a1ec-05fbd03d6d11"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 27s/step - accuracy: 0.0111 - loss: 3.7318 - val_accuracy: 0.0000e+00 - val_loss: 3.5611\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 24s/step - accuracy: 0.1543 - loss: 3.1776 - val_accuracy: 0.0435 - val_loss: 3.5763\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 28s/step - accuracy: 0.2845 - loss: 2.8876 - val_accuracy: 0.0870 - val_loss: 3.6157\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27s/step - accuracy: 0.3779 - loss: 2.7158 - val_accuracy: 0.0870 - val_loss: 3.7108\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 29s/step - accuracy: 0.5529 - loss: 2.4003 - val_accuracy: 0.0870 - val_loss: 3.7980\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 29s/step - accuracy: 0.6096 - loss: 2.1504 - val_accuracy: 0.0435 - val_loss: 3.8270\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 25s/step - accuracy: 0.6403 - loss: 1.9890 - val_accuracy: 0.0435 - val_loss: 3.8120\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 26s/step - accuracy: 0.7100 - loss: 1.7600 - val_accuracy: 0.0000e+00 - val_loss: 3.8318\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 26s/step - accuracy: 0.6921 - loss: 1.6010 - val_accuracy: 0.0435 - val_loss: 3.8603\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 28s/step - accuracy: 0.7433 - loss: 1.4458 - val_accuracy: 0.0000e+00 - val_loss: 3.8665\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ab742e30b50>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(test_data)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "accuracy = accuracy_score(test_labels, y_pred_classes)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Qu4Xf-VNCa",
        "outputId": "776766a4-7844-495a-c1b5-643e66d8d90d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "Test Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now, we have to deploy above code in streamlit to check various images to get output as it matches our input or not\n",
        "\n",
        "# Install Streamlit if not already installed\n",
        "!pip install streamlit\n",
        "\n",
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ... (Your existing code for data loading, preprocessing, model training) ...\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Face Recognition App\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Read the image\n",
        "    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "    img = cv2.imdecode(file_bytes, 1)\n",
        "\n",
        "    # Face detection\n",
        "    faces = detect_faces(img)  # Assuming 'detect_faces' function is defined\n",
        "\n",
        "    if len(faces) > 0:\n",
        "      for i, face in enumerate(faces):\n",
        "        preprocessed_face = preprocess_image(face)  # Assuming 'preprocess_image' function is defined\n",
        "        preprocessed_face = np.expand_dims(preprocessed_face, axis=0)  # Add batch dimension\n",
        "        prediction = model.predict(preprocessed_face)\n",
        "        predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "        st.image(face, caption=f\"Detected Face {i+1}\", use_column_width=True)\n",
        "        st.write(f\"Prediction: {predicted_label}\")\n",
        "    else:\n",
        "        st.write(\"No faces detected in the image.\")\n",
        "else:\n",
        "    st.write(\"Please upload an image.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVC31fpkZ2EH",
        "outputId": "90d6b369-81b2-49cf-c1c9-e409f9a05e73"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.1 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-23 09:46:20.596 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.710 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-11-23 09:46:20.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-23 09:46:20.732 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}